{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3IBGMI51WYn"
      },
      "source": [
        "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
        "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q_EdVhXS1WYs"
      },
      "outputs": [],
      "source": [
        "#%pip install pandas\n",
        "#%pip install matplotlib\n",
        "# add commented pip installation lines for packages used as shown above for ease of testing\n",
        "# the line should be of the format %pip install PACKAGE_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZE05LZS1WYu"
      },
      "source": [
        "## **DO NOT CHANGE** the filepath variable\n",
        "##### Instead, create a folder named 'data' in your current working directory and\n",
        "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GVtCQBrT1WYv"
      },
      "outputs": [],
      "source": [
        "# Can have as many cells as you want for code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "filepath = \"./data/catB_train.parquet\"\n",
        "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQqt38TA1WYw"
      },
      "source": [
        "### **ALL** Code for machine learning and dataset analysis should be entered below.\n",
        "##### Ensure that your code is clear and readable.\n",
        "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hOg7MBWP1WYx",
        "outputId": "607684cf-dd40-440a-904b-4054155b2fcc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/catB_train.parquet'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    273\u001b[0m         path_or_handle,\n\u001b[1;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    278\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/catB_train.parquet'"
          ]
        }
      ],
      "source": [
        "df = pd.read_parquet(filepath)\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnJtFE5B1WYy"
      },
      "source": [
        "To get rid of the missing values in our target column, we replaced missing values with 0 instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ_qaRl81WYy"
      },
      "outputs": [],
      "source": [
        "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGokCpHi1WYz",
        "outputId": "9f90f920-e13e-47fe-af32-3fbb802f43cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17992, 222)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the threshold for missing values (50%)\n",
        "threshold = 0.5 * len(df)\n",
        "\n",
        "# Drop columns with 50% or more missing values\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "df.shape #dropped 82 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O_8q8Gm1WYz",
        "outputId": "24e59a6f-6af1-4cdd-f8aa-2c439194d8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['clntnum', 'race_desc', 'ctrycode_desc', 'clttype', 'stat_flag', 'min_occ_date', 'cltdob_fix', 'cltsex_fix', 'hh_20', 'pop_20', 'hh_size_est', 'annual_income_est', 'ape_gi_42e115', 'ape_ltc_1280bf', 'ape_grp_6fc3e6', 'ape_grp_de05ae', 'ape_inv_dcd836', 'ape_grp_945b5a', 'ape_grp_6a5788', 'ape_ltc_43b9d5', 'ape_grp_9cdedf', 'ape_lh_d0adeb', 'ape_grp_1581d7', 'ape_grp_22decf', 'ape_lh_507c37', 'ape_lh_839f8a', 'ape_inv_e9f316', 'ape_gi_a10d1b', 'ape_gi_29d435', 'ape_grp_caa6ff', 'ape_grp_fd3bfb', 'ape_lh_e22a6a', 'ape_grp_70e1dd', 'ape_grp_e04c3a', 'ape_grp_fe5fb8', 'ape_gi_856320', 'ape_grp_94baec', 'ape_gi_058815', 'ape_grp_e91421', 'ape_lh_f852af', 'ape_lh_947b15', 'ape_32c74c', 'sumins_gi_42e115', 'sumins_ltc_1280bf', 'sumins_grp_6fc3e6', 'sumins_grp_de05ae', 'sumins_inv_dcd836', 'sumins_grp_945b5a', 'sumins_grp_6a5788', 'sumins_ltc_43b9d5', 'sumins_grp_9cdedf', 'sumins_lh_d0adeb', 'sumins_grp_1581d7', 'sumins_grp_22decf', 'sumins_lh_507c37', 'sumins_inv_e9f316', 'sumins_gi_a10d1b', 'sumins_gi_29d435', 'sumins_grp_caa6ff', 'sumins_grp_fd3bfb', 'sumins_lh_e22a6a', 'sumins_grp_70e1dd', 'sumins_grp_e04c3a', 'sumins_grp_fe5fb8', 'sumins_gi_856320', 'sumins_grp_94baec', 'sumins_gi_058815', 'sumins_grp_e91421', 'sumins_lh_f852af', 'sumins_lh_947b15', 'sumins_32c74c', 'prempaid_gi_42e115', 'prempaid_ltc_1280bf', 'prempaid_grp_6fc3e6', 'prempaid_grp_de05ae', 'prempaid_inv_dcd836', 'prempaid_grp_945b5a', 'prempaid_grp_6a5788', 'prempaid_ltc_43b9d5', 'prempaid_grp_9cdedf', 'prempaid_lh_d0adeb', 'prempaid_grp_1581d7', 'prempaid_grp_22decf', 'prempaid_lh_507c37', 'prempaid_lh_839f8a', 'prempaid_inv_e9f316', 'prempaid_gi_a10d1b', 'prempaid_gi_29d435', 'prempaid_grp_caa6ff', 'prempaid_grp_fd3bfb', 'prempaid_lh_e22a6a', 'prempaid_grp_70e1dd', 'prempaid_grp_e04c3a', 'prempaid_grp_fe5fb8', 'prempaid_gi_856320', 'prempaid_grp_94baec', 'prempaid_gi_058815', 'prempaid_grp_e91421', 'prempaid_lh_f852af', 'prempaid_lh_947b15', 'prempaid_32c74c', 'ape_839f8a', 'ape_e22a6a', 'ape_d0adeb', 'ape_c4bda5', 'ape_ltc', 'ape_507c37', 'ape_gi', 'sumins_839f8a', 'sumins_e22a6a', 'sumins_d0adeb', 'sumins_c4bda5', 'sumins_ltc', 'sumins_507c37', 'sumins_gi', 'prempaid_839f8a', 'prempaid_e22a6a', 'prempaid_d0adeb', 'prempaid_c4bda5', 'prempaid_ltc', 'prempaid_507c37', 'prempaid_gi', 'n_months_last_bought_839f8a', 'n_months_last_bought_e22a6a', 'n_months_last_bought_d0adeb', 'n_months_last_bought_c4bda5', 'n_months_last_bought_ltc', 'n_months_last_bought_507c37', 'n_months_last_bought_gi', 'n_months_last_bought_ltc_1280bf', 'n_months_last_bought_grp_6fc3e6', 'n_months_last_bought_grp_de05ae', 'n_months_last_bought_inv_dcd836', 'n_months_last_bought_grp_945b5a', 'n_months_last_bought_grp_6a5788', 'n_months_last_bought_ltc_43b9d5', 'n_months_last_bought_grp_9cdedf', 'n_months_last_bought_lh_d0adeb', 'n_months_last_bought_grp_1581d7', 'n_months_last_bought_grp_22decf', 'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_839f8a', 'n_months_last_bought_inv_e9f316', 'n_months_last_bought_grp_caa6ff', 'n_months_last_bought_grp_fd3bfb', 'n_months_last_bought_lh_e22a6a', 'n_months_last_bought_grp_70e1dd', 'n_months_last_bought_grp_e04c3a', 'n_months_last_bought_grp_fe5fb8', 'n_months_last_bought_grp_94baec', 'n_months_last_bought_grp_e91421', 'n_months_last_bought_lh_f852af', 'n_months_last_bought_lh_947b15', 'n_months_last_bought_32c74c']\n"
          ]
        }
      ],
      "source": [
        "# Check which columns have object data types\n",
        "print(df.select_dtypes(include='object').columns.tolist())\n",
        "\n",
        "# Drop clntnum, ctrycode and date columns\n",
        "df = df.drop(columns=['clntnum', 'min_occ_date', 'cltdob_fix', 'ctrycode_desc'])\n",
        "df_columns_obj = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# fill missing obj values with mode\n",
        "for title in df_columns_obj:\n",
        "  df[title] = df[title].fillna(df[title].mode()[0])\n",
        "\n",
        "### Manually change the object datatypes to integers / floats types\n",
        "# all ape/sumins/prempaid > float\n",
        "to_float = [col for col in df.columns if 'ape' in col or 'sumins' in col or 'prempaid' in col]\n",
        "df[to_float] = df[to_float].astype(float)\n",
        "\n",
        "# all n_months, hh_20, pop_20 > int\n",
        "to_int = [col for col in df.columns if 'n_months' in col]\n",
        "to_int.extend(['hh_20','pop_20'])\n",
        "df[to_int] = df[to_int].astype('int64')\n",
        "\n",
        "# race, clnt type, stat, sex > nominal, one hot encoding\n",
        "to_one_hot = ['race_desc', 'clttype', 'stat_flag', 'cltsex_fix']\n",
        "df = pd.get_dummies(df, columns=to_one_hot)\n",
        "\n",
        "# hh_size, annual income > ordinal encoding\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "to_ordinal = ['hh_size_est', 'annual_income_est']\n",
        "categories = [['0','1', '2', '3', '4', '>4'], ['E.BELOW30K','D.30K-60K','C.60K-100K' ,'B.100K-200K','A.ABOVE200K']]\n",
        "encoder = OrdinalEncoder(categories=categories)\n",
        "df[to_ordinal] = encoder.fit_transform(df[to_ordinal])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2gSqkN51WY0"
      },
      "outputs": [],
      "source": [
        "# Identifiy numeric columns and fill null values with the median value\n",
        "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
        "\n",
        "y = df[\"f_purchase_lh\"]\n",
        "X = df.drop(columns=['f_purchase_lh'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwKWyK-_1WY1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[df.columns] = scaler.fit_transform(df[df.columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_LN6w1y1WY2",
        "outputId": "4527ccd8-6bc4-4495-acc2-54d5eca27fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: Counter({0.0: 13809, 1.0: 584})\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "print('Before:', Counter(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwn34Cxy1WY2"
      },
      "outputs": [],
      "source": [
        "# Finalizing selected features (selected based on consistency across methods and domain knowledge)\n",
        "selected = [\"n_months_last_bought_gi\",\n",
        "                  \"is_valid_email\",\n",
        "                  \"n_months_last_bought_products\",\n",
        "                  \"f_ever_bought_gi\",\n",
        "                  \"is_valid_dm\",\n",
        "                  \"f_mindef_mha\",\n",
        "                  \"f_retail\",\n",
        "                  \"hh_size\",\n",
        "                  \"pop_20\" ]\n",
        "X_train = X_train[selected]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6afeJ9s1WY3"
      },
      "outputs": [],
      "source": [
        "X_val = X_val[selected]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k07gsIIM1WY4",
        "outputId": "70e87a08-d4b2-4a21-9a04-6e16ca7d664d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.9602667407613226\n",
            "Random Forest Confusion Matrix:\n",
            "[[3445   28]\n",
            " [ 115   11]]\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      3473\n",
            "         1.0       0.28      0.09      0.13       126\n",
            "\n",
            "    accuracy                           0.96      3599\n",
            "   macro avg       0.62      0.54      0.56      3599\n",
            "weighted avg       0.94      0.96      0.95      3599\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
        "conf_matrix_rf = confusion_matrix(y_val, y_pred_rf)\n",
        "classification_rep_rf = classification_report(y_val, y_pred_rf)\n",
        "\n",
        "print(f'Random Forest Accuracy: {accuracy_rf}')\n",
        "print(f'Random Forest Confusion Matrix:\\n{conf_matrix_rf}')\n",
        "print(f'Random Forest Classification Report:\\n{classification_rep_rf}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlRoXhh1WY4"
      },
      "source": [
        "## The cell below is **NOT** to be removed\n",
        "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list).\n",
        "##### It is recommended to test the function out prior to submission\n",
        "-------------------------------------------------------------------------------------------------------------------------------\n",
        "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
        "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEYmT8z71WY5"
      },
      "outputs": [],
      "source": [
        "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
        "\n",
        "    df = hidden_data\n",
        "    df = df.drop(columns=['clntnum', 'min_occ_date', 'cltdob_fix', 'ctrycode_desc'])\n",
        "    df_columns_obj = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# fill missing obj values with mode\n",
        "    for title in df_columns_obj:\n",
        "        df[title] = df[title].fillna(df[title].mode()[0])\n",
        "\n",
        "### Manually change the object datatypes to integers / floats types\n",
        "# all ape/sumins/prempaid > float\n",
        "    to_float = [col for col in df.columns if 'ape' in col or 'sumins' in col or 'prempaid' in col]\n",
        "    df[to_float] = df[to_float].astype(float)\n",
        "\n",
        "# all n_months, hh_20, pop_20 > int\n",
        "    to_int = [col for col in df.columns if 'n_months' in col]\n",
        "    to_int.extend(['hh_20','pop_20'])\n",
        "    df[to_int] = df[to_int].astype('int64')\n",
        "\n",
        "# race, clnt type, stat, sex > nominal, one hot encoding\n",
        "    to_one_hot = ['race_desc', 'clttype', 'stat_flag', 'cltsex_fix']\n",
        "    df = pd.get_dummies(df, columns=to_one_hot)\n",
        "\n",
        "# hh_size, annual income > ordinal encoding\n",
        "    from sklearn.preprocessing import OrdinalEncoder\n",
        "    to_ordinal = ['hh_size_est', 'annual_income_est']\n",
        "    categories = [['0','1', '2', '3', '4', '>4'], ['E.BELOW30K','D.30K-60K','C.60K-100K' ,'B.100K-200K','A.ABOVE200K']]\n",
        "    encoder = OrdinalEncoder(categories=categories)\n",
        "    df[to_ordinal] = encoder.fit_transform(df[to_ordinal])\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "    df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    selected = [\"n_months_last_bought_gi\",\n",
        "                  \"is_valid_email\",\n",
        "                  \"n_months_last_bought_products\",\n",
        "                  \"f_ever_bought_gi\",\n",
        "                  \"is_valid_dm\",\n",
        "                  \"f_mindef_mha\",\n",
        "                  \"f_retail\",\n",
        "                  \"hh_size\",\n",
        "                  \"pop_20\" ]\n",
        "    X_train = X_train[selected]\n",
        "\n",
        "    X_val = X_val[selected]\n",
        "\n",
        "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
        "    \n",
        "    rf_model = RandomForestClassifier(random_state=42)\n",
        "    rf_model.fit(X_train)\n",
        "\n",
        "    y_pred_rf = rf_model.predict(X_val)\n",
        "    '''DO NOT REMOVE THIS FUNCTION.\n",
        "\n",
        "The function accepts a dataframe as input and return an iterable (list)\n",
        "of binary classes as output.\n",
        "\n",
        "The function should be coded to test on hidden data\n",
        "and should include any preprocessing functions needed for your model to perform.\n",
        "\n",
        "All relevant code MUST be included in this function.'''\n",
        "    result = [y_pred_rf]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcIL6CKc1WY5"
      },
      "source": [
        "##### Cell to check testing_hidden_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VEms_Wv1WY5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This cell should output a list of predictions.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(filepath)\n\u001b[1;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_purchase_lh\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(testing_hidden_data(test_df))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# This cell should output a list of predictions.\n",
        "test_df = pd.read_parquet(filepath)\n",
        "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
        "print(testing_hidden_data(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cByJEmWD1WY6"
      },
      "source": [
        "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
